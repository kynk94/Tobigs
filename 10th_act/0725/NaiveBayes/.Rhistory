setwd("~/")
rm(list=ls())
# 데이터
library(MASS)
data(Boston)
str(Boston)
##### 회귀분석하기 #####
### medv(본인 소유의 주택가격(중앙값) 단위: $1,000)을 예측해주세요
#medv
fit1=lm(medv~., Boston)
summary(fit1)
library(car)
vif(fit1) #5~10이면 크다.
fit.con <- lm(medv~ 1, Boston)
#fit.step = step(fit1,direction = 'both')
fit.step = step(fit.con, list(lower=fit.con, upper=fit1), direction = 'both')
summary(fit.step) #indus, age가 지워졌다. chas도 지우고 따로 비교해야 하지 않을까?
vif(fit.step)
outlierTest(fit.step)
influence.measures(fit.step)
influencePlot(fit.step, id.method="identify", main="Influence Plot", sub="Circle size")
influenceIndexPlot(fit.step, id.n=3)
# 데이터
library(MASS)
data(Boston)
str(Boston)
rm(list=ls())
# 데이터
library(MASS)
data(Boston)
str(Boston)
##### 회귀분석하기 #####
### medv(본인 소유의 주택가격(중앙값) 단위: $1,000)을 예측해주세요
#medv
fit1=lm(medv~., Boston)
summary(fit1)
#F는 연구모델의 적합성, P는 가설의 유의성 T는 변수들의 영향 * 갯수가 중요도를 의미
#분포곡선에서 x축에 T,F값이 존재하고 P는 T,F보다 큰 부분의 면적을 의미 -> 데이터연관성이 99.5%안에 들어야 유의미하다는 뜻이었네.
#T=Estimate/Std.Error 즉 Estimate에 비해 Error가 클수록 의미가 없다.
#sumary의 마지막부분인 Pr은 T이상인 부분의 면적. 면적이 작을수록 유의미.
#F는?
#R-squared가 0.7406이니까 74%를 설명한다는 것
#마지막 p-value는 모델에대한 F 검정의 P값, 이 경우 2.2e-16
plot(Boston$crim, Boston$medv)    #T  -3.287
library(car)
vif(fit1) #5~10이면 크다.
fit.con <- lm(medv~ 1, Boston)
#fit.step = step(fit1,direction = 'both')
fit.step = step(fit.con, list(lower=fit.con, upper=fit1), direction = 'both')
summary(fit.step) #indus, age가 지워졌다. chas도 지우고 따로 비교해야 하지 않을까?
vif(fit.step)
outlierTest(fit.step)
influence.measures(fit.step)
influencePlot(fit.step, id.method="identify", main="Influence Plot", sub="Circle size")
influencePlot(fit.step, id.method="identify", main="Influence Plot", sub="Circle size is portional to Cook's distace")
par(mforw=c(1,1))
influencePlot(fit.step, id.method="identify", main="Influence Plot", sub="Circle size is portional to Cook's distace")
par(mforw=c(1,1))
influencePlot(fit.step, id.method="identify", main="Influence Plot", sub="Circle size is portional to Cook's distace")
par(mfrow=c(2,2))
influencePlot(fit.step, id.method="identify", main="Influence Plot", sub="Circle size is portional to Cook's distace")
rm(list=ls())
# 데이터
library(MASS)
data(Boston)
str(Boston)
##### 회귀분석하기 #####
### medv(본인 소유의 주택가격(중앙값) 단위: $1,000)을 예측해주세요
#medv
fit1=lm(medv~., Boston)
summary(fit1)
library(car)
vif(fit1) #5~10이면 크다.
fit.con <- lm(medv~ 1, Boston)
#fit.step = step(fit1,direction = 'both')
fit.step = step(fit.con, list(lower=fit.con, upper=fit1), direction = 'both')
summary(fit.step) #indus, age가 지워졌다. chas도 지우고 따로 비교해야 하지 않을까?
vif(fit.step)
outlierTest(fit.step)
influence.measures(fit.step)
influencePlot(fit.step, id.method="identify", main="Influence Plot", sub="Circle size is portional to Cook's distace")
influenceIndexPlot(fit.step, id.n=3)
# 데이터
library(MASS)
data(Boston)
rm(list=ls())
# 데이터
library(MASS)
data(Boston)
str(Boston)
##### 회귀분석하기 #####
### medv(본인 소유의 주택가격(중앙값) 단위: $1,000)을 예측해주세요
#medv
fit1=lm(medv~., Boston)
summary(fit1)
fit.con <- lm(medv~ 1, Boston)
#fit.step = step(fit1,direction = 'both')
fit.step = step(fit.con, list(lower=fit.con, upper=fit1), direction = 'both')
outlierTest(fit.step)
influence.measures(fit.step)
influencePlot(fit.step, id.method="identify", main="Influence Plot", sub="Circle size is portional to Cook's distace")
influenceIndexPlot(fit.step, id.n=3)
install.packages("kknn")
install.packages("class")
install.packages("tm")
install.packages("SnowballC")
install.packages("wordcloud")
install.packages("naivebayes")
rm(list=ls())
### packages
if(!require(class)) install.packages("class"); library(class) # for knn
if(!require(kknn)) install.packages("kknn"); library(kknn) # for wknn
if(!require(caret)) install.packages("caret"); library(caret)
if(!require(e1071)) install.packages("e1071"); library(e1071)
if(!require(dplyr)) install.packages("dplyr"); library(dplyr)
rm(list=ls())
# 유방암 진단 결과 데이터 셋
### setwd
setwd("C:/future/toBigs/10기/2주차/KNN LDA/data")
setwd("~/Tobigs/0725/KNN LDA/script")
rm(list=ls())
### packages
if(!require(class)) install.packages("class"); library(class) # for knn
if(!require(kknn)) install.packages("kknn"); library(kknn) # for wknn
if(!require(caret)) install.packages("caret"); library(caret)
if(!require(e1071)) install.packages("e1071"); library(e1071)
if(!require(dplyr)) install.packages("dplyr"); library(dplyr)
### load data
# stringAsFactors = F: read.csv() 함수는 읽어올 때 모든 chr를 factor로 변환하는데
# 이를 자동으로 바꾸지 않고 chr 그대로 읽어오는 것
wdbc <- read.csv('wisc_bc_data.csv', stringsAsFactors = F)
setwd("~/Tobigs/0725/KNN LDA")
### load data
# stringAsFactors = F: read.csv() 함수는 읽어올 때 모든 chr를 factor로 변환하는데
# 이를 자동으로 바꾸지 않고 chr 그대로 읽어오는 것
wdbc <- read.csv('data/wisc_bc_data.csv', stringsAsFactors = F)
str(wdbc) # 자료구조 확인
dim(wdbc) # 569 x 32
# 불필요한 id 변수 제거
wdbc <- wdbc[-1]
str(wdbc)
# 종속변수(y) 분포 확인
table(wdbc$diagnosis)
prop.table(table(wdbc$diagnosis))
# 종속변수(y)인 diagnosis 변수 factor형으로 변환
wdbc$diagnosis <- factor(wdbc$diagnosis, level=c("B","M"))
# set.seed(1) : 난수 고정
set.seed(1)
# createDataPartition
# caret패키지 안에 있는 데이터 파티션, 층화추출
idx <- createDataPartition(y = wdbc$diagnosis, p = 0.7, list =FALSE)
wdbc_train <- wdbc[idx,]
wdbc_test <- wdbc[-idx,]
### 기본 knn (majority voting, 다수결) ###
# 임의의 홀수 k 를 넣어보자
# y를 지우고 넣은것들임, diagnosis 가 y값
wdbc_train %>% str
model.knn <- knn(wdbc_train[-1],wdbc_test[-1], wdbc_train$diagnosis, k=5)
confusionMatrix(model.knn, wdbc_test$diagnosis) # Specificity : 0.9048
### weighted knn (가중치) ###
# distance, kernel(가중치)
# 포뮬러, 트레인, 테스트, scale=T는 디폴트 정규화 해주는 것
# scale = T하면 표준화
model.wknn <- kknn(diagnosis ~., wdbc_train, wdbc_test, k=5, scale = F)
# 속할 확률
model.wknn$prob
# 최근접 k개와의 거리
model.wknn$D
# wknn으로 예측한 값
wknn_pred <- model.wknn$fitted.values
confusionMatrix(wknn_pred, wdbc_test$diagnosis) # Specificity : 0.9524
### Feature scaling ###
summary(wdbc)
## min-max 스케일링
normalize <- function(x){
return( (x-min(x))/(max(x)-min(x)) )
}
colnames(wdbc)
wdbc_normal <- as.data.frame(lapply(wdbc[-1], normalize))
summary(wdbc_normal) #  0~1 사이의 값으로 바뀜
wdbc_normal$diagnosis <- wdbc$diagnosis
wdbc_train_n <- wdbc_normal[idx,]
wdbc_test_n <- wdbc_normal[-idx,]
### 기본 knn (majority voting) ###
# 임의의 홀수 k 를 넣어보자
wdbc_pred_n <- knn(wdbc_train_n[-31],wdbc_test_n[-31], wdbc_train_n$diagnosis, k=5)
confusionMatrix(wdbc_pred_n, wdbc_test$diagnosis) # Specificity : 0.9524
### weighted knn (가중치) ###
# distance, kernel(가중치)
wknn.model <- kknn(diagnosis ~., wdbc_train_n, wdbc_test_n, k=5, scale = F)
wdbc_pred_n2 <- wknn.model$fitted.values
confusionMatrix(wdbc_pred_n2, wdbc_test$diagnosis) # Specificity : 0.9524
### 최적의 K를 구하는 방법 ###
# Cross-Validation
# ks = 실험할 k, 일부러 홀수로 지정하였다.
wdbc.cv <- train.kknn(diagnosis ~., wdbc_train_n,
ks = seq(1, 50, by=2), scale = T);wdbc.cv
best_k <- wdbc.cv$best.parameters$k;best_k
setwd("~/Tobigs/0725/NaiveBayes")
setwd("~/Tobigs/0725/NaiveBayes")
rm(list=ls())
## Window 환경에서 UTF-8로 인코딩된 csv파일을 read.csv로 읽는 경우
## EOF within quoted string 이라는 Warning이 발생하기도한다
## 이 문제를 해결하기 위해 아래 코드를 실행하여 시스템 기본 locale을 바꿔주면 해결된다!
##Sys.setlocale(category="LC_CTYPE", locale="C")
setwd("~/Tobigs/0725/NaiveBayes")
sms_raw <-read.csv("sms_spam.csv",stringsAsFactors = FALSE)
sms_raw <-read.csv('sms_spam.csv',stringsAsFactors = FALSE)
## Window 환경에서 UTF-8로 인코딩된 csv파일을 read.csv로 읽는 경우
## EOF within quoted string 이라는 Warning이 발생하기도한다
## 이 문제를 해결하기 위해 아래 코드를 실행하여 시스템 기본 locale을 바꿔주면 해결된다!
Sys.setlocale(category="LC_CTYPE", locale="C")
sms_raw <-read.csv('sms_spam.csv',stringsAsFactors = FALSE)
head(sms_raw)
## 5574개의 메시지와 두 속성(햄/스팸)으로 구성
## type변수가 chr 형으로 저장되어 있는데, factor 변수로 변경해야 한다.
## 13.4%가 스팸메시지이다!!
str(sms_raw)
sms_raw$type<-factor(sms_raw$type)
## 5574개의 메시지와 두 속성(햄/스팸)으로 구성
## type변수가 chr 형으로 저장되어 있는데, factor 변수로 변경해야 한다.
## 13.4%가 스팸메시지이다!!
str(sms_raw)
table(sms_raw$type)
## 메시지는 문자열로 구성되어있다.
## 여러 접속사 등이 포함되어 처리를 해야되고, 문장을 개별 단어로도 분리해야한다
## tm 패키지를 불러와 함수를 사용하자!
## install.packages("tm")
library(tm)
## 메시지는 문자열로 구성되어있다.
## 여러 접속사 등이 포함되어 처리를 해야되고, 문장을 개별 단어로도 분리해야한다
## tm 패키지를 불러와 함수를 사용하자!
## install.packages("tm")
library(tm)
## step1.Corpus(말뭉치)를 만들자
## corpus란 텍스트 문서의 모음을 말함
## VCorpus()는 corpus를 만들어서 메모리상에 저장한다
## PCorpus()는 corpus를 직접 디스크에 저장한다
## 말뭉치가 5574개의 메시지에 대한 문서를 포함하고있다.
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
## 말뭉치 내부의 메시지 내용을 보기위해서는 as.character()를 이용해야한다
as.character(sms_corpus[[1]])
## step2. Corpus내부의 텍스트를 정제하자
## tm_map()를 이용해 말뭉치를 mapping한다
## 우선 tolower()를 이용해 모든 문자를 소문자로 변환하고
## mapping한 결과물을 corpus_clean에 저장하자!!
sms_corpus_clean<-tm_map(sms_corpus, content_transformer(tolower))
as.character(sms_corpus_clean[[1]])
## removeNumbers()를 이용해서 숫자도 제거하도록하자
sms_corpus_clean<-tm_map(sms_corpus_clean, removeNumbers)
## to, and, but, or 같은 단어를 제거하자(stop word)
## 자주 등장하지만 의미있는 정보를 제공하지 않기 때문에 미련없이 제거ㅎㅎ
## head를 쓰면 stopword로 분류되는 단어들이 뭐가 있는지 볼 수 있다
## stopword가 174개 존재하는군
head(stopwords(), 20)
length(stopwords())
class(stopwords())
sms_corpus_clean<-tm_map(sms_corpus_clean, removeWords, stopwords())
## 구두점도 제거하자(구두점 다들 뭔지 아시죠? ㅎㅎ)
sms_corpus_clean<-tm_map(sms_corpus_clean, removePunctuation)
## step3. 어간 추출(stemming)
## learning, learns, learned 가 있을 때 모두 learn 이된다
## Snowball 패키지의 wordStem()을 사용하자
## install.packages("SnowballC")
library(SnowballC)
sms_corpus_clean<-tm_map(sms_corpus_clean, stemDocument)
as.character(sms_corpus_clean[[1]])
sms_corpus_clean<-tm_map(sms_corpus_clean, stemDocument)
## 의미없는 한 줄띄기도 제거해주자
sms_corpus_clean<-tm_map(sms_corpus_clean, stripWhitespace)
as.character(sms_corpus_clean[[1]])
## 구두점도 제거하자(구두점 다들 뭔지 아시죠? ㅎㅎ)
sms_corpus_clean<-tm_map(sms_corpus_clean, removePunctuation)
## step4. 토큰화(tokenization)
## tm 패키지의 DocumentTermMatrix()를 이용해 말뭉치를 입력받는다.
## 위의 output은 sparse matrix로 나온다.
## 행은 메시지, 열은 단어임!!
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
dim(sms_dtm)
## 이제 train/test data를 나눌거에요
sms_dtm_train<-sms_dtm[1:4169, ]
sms_dtm_test<-sms_dtm[4170:5572, ]
## 각 메시지가 스팸인지 햄인지 나타내는지 따로 벡터에 저장할게요
## 저장한 문자열 벡터에 스팸의 비율이 적당한지 prop.table로 확인할게요
## 오 그냥 짤랐는데 스팸 비율이 환상적이에요!! OMG!!
sms_train_labels<-sms_raw[1:4169,]$type
sms_test_labels<-sms_raw[4170:5572,]$type
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
## step5. 워드클라우드(Word Cloud)
## 텍스트 데이터에서 단어의 빈도를 시작적으로 묘사하는 방법이에요
## install.packages("wordcloud")
## random.order = FALSE는 자주 나오는 단어일 수록 가운데 위치하게 해줍니다
## min.freq는 클라우드에 등장하기 위한 최소빈도수를 나타내는데
## 말뭉치의 약 10%로 설정하는게 보통이에요
library(wordcloud)
## step5. 워드클라우드(Word Cloud)
## 텍스트 데이터에서 단어의 빈도를 시작적으로 묘사하는 방법이에요
## install.packages("wordcloud")
## random.order = FALSE는 자주 나오는 단어일 수록 가운데 위치하게 해줍니다
## min.freq는 클라우드에 등장하기 위한 최소빈도수를 나타내는데
## 말뭉치의 약 10%로 설정하는게 보통이에요
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
## subset()함수를 이용해서 sms_raw 데이터에서 스팸과 햄을 나눌게요
## 나눈 데이터에 대해 각각 워드클라우드 만들게요
## spam은 call, now, free, mobile, stop과 같은 단어 등장
## ham은 can, get, like, good, got, come, know와 같은 단어 등장
## 이러한 차이는 NBC 모델이 카테고리를 구별할 수 있는 키워드가 됩니다
spam <-subset(sms_raw, type=="spam")
ham <-subset(sms_raw, type=="ham")
wordcloud(spam$text, min.freq = 30, scale=c(3,0,5), random.order = FALSE)
wordcloud(ham$text, min.freq = 30, scale=c(3,0,5), random.order = FALSE)
## step6. sparse matrix 구조 변환하기
## 위에서 만든 sparse matrix를 nbc가 훈련하는데 용이하도록 바꿔줄거에요
## sms_dtm이 6597열을 갖고 있다고 했잖아요?
## 좀 줄이기 위해서 빈도가 5 미만이면 삭제할게요
## 5번 이상 나타난 단어들의 문자 벡터를 sms_freq_words에 저장합니다
## str을 해보니 1158개 단어가 있음을 알 수 있군요!!
sms_freq_words<-findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
## 위에서 만든 train/test에서 sms_freq_words에 있는 칼럼만 추출할게요
## 그럼 이제 train/test data는 1158 개 단어만 갖고 있는거에요
sms_dtm_freq_train<-sms_dtm_train[,sms_freq_words]
sms_dtm_freq_test<-sms_dtm_test[,sms_freq_words]
## NBC는 단어의 빈도는 고려하지 않고 등장여부가 중요해요
## 그런데 sparse matrix는 apple단어가 2번 등장하면 2가 저장되어있습니다.
## 이렇게되면 nbc는 훈련을 할 수가 없어요.
## 그래서 빈도가 0이면 NO, 1이상이면 Yes를 출력하는 함수를 만들게요
convert_counts<-function(x){
x<-ifelse(x>0, "Yes","No")
}
## 위 함수를 일괄 적용하기 위해서 apply함수를 쓸게요
## 옵션에 MARGIN=2는 conver_counts 함수를 각 열에 대해서 적용한다는 뜻!
sms_train<-apply(sms_dtm_freq_train, MARGIN=2, convert_counts)
sms_test<-apply(sms_dtm_freq_test, MARGIN=2, convert_counts)
summary(sms_train[,1:5])
## 전처리한 데이터를 갖고 NB모델을 훈련시킬겁니다
## naiveBayes(train, class, laplace=0) 인데
## train은 훈련 데이터이고 class는 훈련 데이터의 라벨 벡터에요
## laplace는 라플라스 스무딩 사용여부에요 휴 길다길어
library(naivebayes)
## 전처리한 데이터를 갖고 NB모델을 훈련시킬겁니다
## naiveBayes(train, class, laplace=0) 인데
## train은 훈련 데이터이고 class는 훈련 데이터의 라벨 벡터에요
## laplace는 라플라스 스무딩 사용여부에요 휴 길다길어
library(naivebayes)
## 전처리한 데이터를 갖고 NB모델을 훈련시킬겁니다
## naiveBayes(train, class, laplace=0) 인데
## train은 훈련 데이터이고 class는 훈련 데이터의 라벨 벡터에요
## laplace는 라플라스 스무딩 사용여부에요 휴 길다길어
library(naivebayes)
## 전처리한 데이터를 갖고 NB모델을 훈련시킬겁니다
## naiveBayes(train, class, laplace=0) 인데
## train은 훈련 데이터이고 class는 훈련 데이터의 라벨 벡터에요
## laplace는 라플라스 스무딩 사용여부에요 휴 길다길어
library(naivebayes)
install.packages("naivebayes")
## ??ó???? ?????͸? ???? NB????�� ?Ʒý?ų?̴ϴ?
## naiveBayes(train, class, laplace=0) ?ε?
## train�� ?Ʒ? ???????̰? class?? ?Ʒ? ???????? ???? ???Ϳ???
## laplace?? ???ö??? ?????? ???뿩?ο??? ?? ???ٱ???
library(naivebayes)
sms_classifier<-naive_bayes(sms_train, sms_train_labels, laplace = 1)
install.packages("e1071")
sms_classifier<-naive_bayes(sms_train, sms_train_labels, laplace = 1)
sms_classifier<-naivebayes(sms_train, sms_train_labels, laplace = 1)
install.packages("e1071")
sms_classifier<-naive_bayes(sms_train, sms_train_labels, laplace = 1)
## 전처리한 데이터를 갖고 NB모델을 훈련시킬겁니다
## naiveBayes(train, class, laplace=0) 인데
## train은 훈련 데이터이고 class는 훈련 데이터의 라벨 벡터에요
## laplace는 라플라스 스무딩 사용여부에요 휴 길다길어
library(naivebayes)
sms_classifier<-naive_bayes(sms_train, sms_train_labels, laplace = 1)
install.packages("e1071")
install.packages("naivebayes")
