{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 (Tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import torchvision.datasets as dsets\n",
    "# import torchvision.transforms as transforms\n",
    "#from torch.autograd import Variable\n",
    "# 파이토치 0.4.0.부터는 Variable 선언을 해주지 않아도 된다!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 초기화되지 않은 5 x 3 행렬\n",
    "x = torch.empty(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2067, 0.1282, 0.4910],\n        [0.9297, 0.0594, 0.8687],\n        [0.1551, 0.9425, 0.7469],\n        [0.6159, 0.0602, 0.8203],\n        [0.1098, 0.4909, 0.7488]])\ntensor([[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 랜덤으로 초기화된 5 x 3행렬\n",
    "x = torch.rand(5,3)\n",
    "# x 에 대한 설명 : 0~1 균등분포\n",
    "print(x)\n",
    "# 0으로 채워지고 long 데이터 타입을 가지는 행렬\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5,3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 존재하는 텐서를 기반으로 새로운 텐서 생성가능\n",
    "# 입력 텐서의 type들이 사용자에 의해 새롭게 제공되지 않는 이상 기존의 값들을 사용\n",
    "x = x.new_ones(5, 3, dtype=torch.double)\n",
    "print(x) # new_* methods take in sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1121, -0.3816, -1.4183],\n        [-0.5780,  0.1633, -0.2289],\n        [ 1.2817, -1.1504, -2.3666],\n        [ 0.0484,  0.1768,  0.1669],\n        [ 0.5859, -0.1288,  0.0841]])\ntorch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn_like(x, dtype=torch.float) # override dtype\n",
    "# x 에 대한 설명 : 평균 0 표준편차 1인 정규분포\n",
    "print(x)      # result has the same size\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연산 (Operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4619,  0.4768, -0.9055],\n        [ 0.4043,  0.9891, -0.2033],\n        [ 1.9780, -0.6412, -1.6208],\n        [ 0.9216,  1.0739,  0.8096],\n        [ 0.9426,  0.5438,  0.3404]])\ntensor([[-0.0644, -0.3276, -0.7274],\n        [-0.5677,  0.1348, -0.0059],\n        [ 0.8924, -0.5858, -1.7649],\n        [ 0.0422,  0.1586,  0.1073],\n        [ 0.2090, -0.0866,  0.0215]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x+y)\n",
    "# print(torch.add(x, y)) # 완전히 같다\n",
    "print(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4619,  0.4768, -0.9055],\n        [ 0.4043,  0.9891, -0.2033],\n        [ 1.9780, -0.6412, -1.6208],\n        [ 0.9216,  1.0739,  0.8096],\n        [ 0.9426,  0.5438,  0.3404]])\n"
     ]
    }
   ],
   "source": [
    "# 더하기 : 파라미터로(결과가 저장되는) 결과 텐서(output tensor) 이용\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 텐서를 제자리에서 변조하는 연산은 _문자를 이용해 postfix(연산자를 피연산자의 뒷쪽에 표시)로 표기한다.\n",
    "* 예를 들면 x.copy_(y)와 x.t_()는 x를 변경시킨다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4619,  0.4768, -0.9055],\n        [ 0.4043,  0.9891, -0.2033],\n        [ 1.9780, -0.6412, -1.6208],\n        [ 0.9216,  1.0739,  0.8096],\n        [ 0.9426,  0.5438,  0.3404]])\n"
     ]
    }
   ],
   "source": [
    "# 더하기 : 제자리(in-place)\n",
    "# adds x to y\n",
    "y.add_(x); print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Size------\ntorch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n----------Tensor Y (1 x 16)----------\ntensor([-0.3853,  1.5593,  0.2423, -1.2236,  1.5524, -2.0325, -0.5227,  1.0139,\n         0.3413, -1.4049, -0.6174, -0.4557,  0.0299, -0.2832,  1.1442, -0.2201])\n----------Tensor Z (2 x 8)----------\ntensor([[-0.3853,  1.5593,  0.2423, -1.2236,  1.5524, -2.0325, -0.5227,  1.0139],\n        [ 0.3413, -1.4049, -0.6174, -0.4557,  0.0299, -0.2832,  1.1442, -0.2201]])\n"
     ]
    }
   ],
   "source": [
    "# Resizing : 텐서 사이즈를 재변경하거나, 모양(shape)을 변경하고 싶다면 view 사용\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16) # 1*16 reshape\n",
    "z = x.view(-1, 8) # ?*8 reshape\n",
    "print('------Size------')\n",
    "print(x.size(), y.size(), z.size())\n",
    "print('----------Tensor Y (1 x 16)----------')\n",
    "print(y)\n",
    "print('----------Tensor Z (2 x 8)----------')\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy 변환 (Bridge)\n",
    "* 토치 텐서와 배열은 근본적으로 메모리 위치를 공유하기 때문에 하나를 변경하면 다른 하나도 변경된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Tensor -> Array\n",
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1) # a를 참조하고 있는 b도 변한다\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\ntensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Array -> Tensor\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서는 .to 메소드를 이용해 (CUDA를 지원하는)그 어떠한 디바이스로도 옮길 수 있다\n",
    "# let us run this cell only if CUDA is available\n",
    "# We will use 'torch.device' objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    y = torch.ones_like(x, device=device)   # a CUDA device object\n",
    "    x = x.to(device)                        # directly create a tensor on GPU\n",
    "    z = x + y                               # or just use strings ''.to('cuda')\n",
    "    print(z)\n",
    "    print(z.to('cpu', torch.double))        # ''.to'' cna also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd : 자동 미분 (Automatic Diff)\n",
    "**autograd** 패키지는 텐서의 모든 연사에 대하여 자동 미분을 제공한다. 이 패키지는 실행 시점에 정의되는(define-by-run) 프레임워크인데 다시 말하면 코드가 어떻게 실행되는지에 따라 역전파(backprop)가 정의되며, 각각의 반복마다 역전파가 달라질 수 있다는 것이다.\n",
    "\n",
    "* 텐서플로는 정의된 다음 실행되는(defined-and-run) 프레임워크인데 이는 그래프 구조에서 미리 조건과 반복을 정의하고 나서 실행된다.\n",
    "* PyTorch를 비롯한 Chainer, DyNet 등은 define-by-run 프레임워크이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.Tensor는 패키지에서 가장 중심이 되는 클래스\n",
    "\n",
    "-- 텐서의 속성 중 하나인 **.requires_grad**를 **True**로 세팅하면, 텐서의 모든 연산에 대해 추적을 시작한다.\n",
    "\n",
    "-- 계산 작업이 모두 수행됐다면 **.backward()** 를 호출하여 모든 그라디언트들을 자동으로 계산할 수 있다.\n",
    "\n",
    "-- 이 텐서를 위한 그라디언트는 **.grad** 속성에 누적되어 저장된다.\n",
    "\n",
    "-- 텐서에 대해 기록(history) 추적을 중지하려면 **.detach()** 를 호출해 현재의 계산 기록으로부터 분리시키고 이후에 일어나는 계산들은 추적되지 않게 할 수 있다.\n",
    "\n",
    "-- 기록 추적(및 메모리 사용)에 대해 방지를 하려면, 코드 블럭을 **with torch.no_grad():** 로 래핑(wrap)할 수 있다. 이는 특히 모델을 평가할 때 엄청난 도움이 되는데, 왜냐하면 모델은 requires_grad=True 속성이 적용된 학습 가능한 파라미터를 가지고 있을 수 있으나 우리는 그라디언트가 필요하지 않기 때문이다.\n",
    "\n",
    "-- 자동 미분을 위해 매우 중요한 클래스가 하나 더 있는데 바로 **Function**이다.\n",
    "\n",
    "-- **Tensor**와 **Function**은 상호 연결되어 있으며, 비순환(비주기) 그래프를 생성하는데, 이 그래프는 계싼 기록 전체에 대하여 인코딩을 수행한다. 각 변수는 **Tensor**를 생성한 **Function**을 참조하는 **.grad_fn** 속성을 가지고 있다. (단, 사용자에 의해 생성된 텐서는 제외한다 ; 해당 텐서들은 **grad_fn** 자체가 **None** 상태이다)\n",
    "\n",
    "-- 만약 도함수(derivatives)들을 계산하고 싶다면, Tensor의 **.backward()** 를 호출하면 된다. 만약 **Tensor**가 스칼라 형태라면, **backward()** 사용에 있어 그 어떠한 파라미터도 필요하지 않는다. 그러나 한 개 이상의 요소를 가지고 있다면 올바른 모양(matching shape)의 텐서인 **gradient** 파라미터를 명시할 필요가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 텐서를 생성하고 requires~ 로 세팅해 계산 추적한다.\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n        [3., 3.]], grad_fn=<AddBackward>)\n<AddBackward object at 0x000001CAF754E9B0>\n"
     ]
    }
   ],
   "source": [
    "y = x+2\n",
    "print(y)\n",
    "# y는 연산의 결과로서 생성된 것이므로 grad_fn을 가지고 있다.\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 9.],\n        [9., 9.]], grad_fn=<ThMulBackward>) tensor(36., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y\n",
    "out = z.sum()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**requires_grad(...)** 은 이미 존재하는 텐서의 **requires_grad** 플래그를 제자리(in-place)에서 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----1st-----\nFalse\n-----2nd-----\nTrue\n-----3rd-----\n<SumBackward0 object at 0x000001CAF753A240>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print('-----1st-----')\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print('-----2nd-----')\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print('-----3rd-----')\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그라디언트 (Gradients)\n",
    "* 다시 돌아와서 이제 **out**은 하나의 스칼라 값을 가지고 있기 때문에 **out.bacward()**는 **out.backward(torch.tensor(1))**와 동등한 결과를 리턴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 6.],\n        [6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)\n",
    "# y = x+2\n",
    "# z = y^2 = x^2 + 4*x + 4\n",
    "# dz/dx = 2*x + 4\n",
    "# then, x.grad = 2*1 + 4 =6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.], grad_fn=<MulBackward>)\ntensor([ 0.6000,  6.0000, 60.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, requires_grad=True)\n",
    "y = x**2\n",
    "z = y*3\n",
    "print(z)\n",
    "gradient = torch.tensor([0.1, 1, 10], dtype=torch.float)\n",
    "# backward 인자가 곱해져 그라디언트 값 출력\n",
    "z.backward(gradient) # * gradient 의 결과\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn과 nn.functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn의 기능들 (weight를 자동 설정해줌, conv2d의 경우)\n",
    "* Parameters    \n",
    "* Linear    \n",
    "* Container    \n",
    "* Dropout    \n",
    "* Conv  \n",
    "* Sparse    \n",
    "* Pooling    \n",
    "* Distance    \n",
    "* Padding    \n",
    "* Loss  \n",
    "* Non-linear Activation    \n",
    "* Vision    \n",
    "* Normalization  \n",
    "* Data Parallel   \n",
    "* Utilities   \n",
    "* Recurrent  \n",
    "\n",
    "### nn.functional의 기능들(외부에서 만든 filter, weight를 넣어야 함)\n",
    "* Conv    \n",
    "* Pooling   \n",
    "* Non-linear activation   \n",
    "* Normalization  \n",
    "* Linear function(=fully connected layer)  \n",
    "* Dropout  \n",
    "* Distance\n",
    "* Loss   \n",
    "* Vision  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]], requires_grad=True)\ntensor([[[[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "#### nn.functional ####\n",
    "inp = torch.ones(1,1,3,3, requires_grad = True)\n",
    "filter = torch.ones(1,1,3,3)\n",
    "print(inp)\n",
    "print(filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----output----\ntensor([[[[9.]]]], grad_fn=<ThnnConv2DBackward>)\n----grad_fn----\n<ThnnConv2DBackward object at 0x000001CAF7537B00>\n----Gradient----\ntensor([[[[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "out = F.conv2d(inp, filter)\n",
    "print(\"----output----\")\n",
    "print(out)\n",
    "out.backward()\n",
    "print(\"----grad_fn----\")\n",
    "print(out.grad_fn)\n",
    "print(\"----Gradient----\")\n",
    "print(inp.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----output----\n",
      "tensor([[[[ 18.]]]])\n",
      "----grad_fn----\n",
      "<ThnnConv2DBackward object at 0x000001CD86020B38>\n",
      "----Gradient----\n",
      "tensor([[[[ 3.,  3.,  3.],\n",
      "          [ 3.,  3.,  3.],\n",
      "          [ 3.,  3.,  3.]]]])\n"
     ]
    }
   ],
   "source": [
    "filter += 1\n",
    "out = F.conv2d(inp, filter)\n",
    "print(\"----output----\")\n",
    "print(out)\n",
    "out.backward()\n",
    "print(\"----grad_fn----\")\n",
    "print(out.grad_fn)\n",
    "print(\"----Gradient----\")\n",
    "print(inp.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0186, -0.1918, -0.0093],\n",
       "          [ 0.2435,  0.0279,  0.1439],\n",
       "          [ 0.1880, -0.2797, -0.0804]]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### nn ####\n",
    "inp = torch.ones(1,1,3,3, requires_grad = True)\n",
    "func = nn.Conv2d(1, 1, 3) # (input_channel, output_channel, filter_size or kernel_size)\n",
    "func.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.3047]]]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "out = func(inp)\n",
    "print(out)\n",
    "# nn.Conv2d는 자동으로 bias를 포함시키기 때문에 값이 위의 weight 총합 값과 다름\n",
    "print(inp.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0186, -0.1918, -0.0093],\n",
      "          [ 0.2435,  0.0279,  0.1439],\n",
      "          [ 0.1880, -0.2797, -0.0804]]]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(inp.grad) # 그냥 weight 자체가 산출됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- ReLU activation ----\ntensor([[[[0.4061, 0.0000, 0.0704],\n          [0.0000, 1.2012, 0.0000],\n          [0.0000, 0.0994, 0.0000]]]])\n---- Max Pooling 2 x 2 by stride 1 ----\ntensor([[[[1.2012, 1.2012],\n          [1.2012, 1.2012]]]])\n---- Sigmoid ----\ntensor([[[[0.7687, 0.7687],\n          [0.7687, 0.7687]]]])\n---- Tanh of Max Pooled image ----\ntensor([[[[0.8340, 0.8340],\n          [0.8340, 0.8340]]]])\n---- Average Pooling 2 x 2 by stride 1 ----\ntensor([[[[0.8340]]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\torch\\nn\\functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\nC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "### 다양한 활성함수들\n",
    "# import torch.nn.functional as F\n",
    "# import torch.nn as nn\n",
    "inp2 = torch.randn(1,1,3,3)\n",
    "a = F.relu(inp2)\n",
    "print(\"---- ReLU activation ----\")\n",
    "print(a)\n",
    "b = nn.MaxPool2d(2, stride=1)\n",
    "print(\"---- Max Pooling 2 x 2 by stride 1 ----\")\n",
    "bb = b(a)\n",
    "print(bb)\n",
    "c = F.sigmoid(bb)\n",
    "print(\"---- Sigmoid ----\")\n",
    "print(c)\n",
    "d = F.tanh(bb)\n",
    "print(\"---- Tanh of Max Pooled image ----\")\n",
    "print(d)\n",
    "e = nn.AvgPool2d(2, stride=1)\n",
    "print(\"---- Average Pooling 2 x 2 by stride 1 ----\")\n",
    "ee = e(d)\n",
    "print(ee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 일련의 과정을 한 번에 클래스로 묶어서 \n",
    "# 처리하도록 직접 모델을 설계한다!\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        self.Max_pool = nn.MaxPool2d(2, stride=1)\n",
    "        self.Avg_pool = nn.AvgPool2d(2, stride=1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(x)\n",
    "        x = self.Max_pool(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.Avg_pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.8340]]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = model()\n",
    "net(inp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model(\n  (Max_pool): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n  (Avg_pool): AvgPool2d(kernel_size=2, stride=1, padding=0)\n)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
