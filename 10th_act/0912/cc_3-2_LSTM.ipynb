{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Models and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ff10093e70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = torch.nn.LSTM(3, 3)  # Input은 3차원, output도 3차원\n",
    "inputs = [torch.autograd.Variable(torch.randn(1, 3))\n",
    "          for _ in range(5)]  # sequence의 길이는 5\n",
    "\n",
    "# Hidden state를 초기화한다.\n",
    "hidden = (torch.autograd.Variable(torch.randn(1, 1, 3)),\n",
    "          torch.autograd.Variable(torch.randn((1, 1, 3))))\n",
    "for i in inputs:\n",
    "    # sequence의 요소 하나씩 단계적으로 진행한다.\n",
    "    # 매 단계마다 hidden은 hidden state 정보를 담게 된다.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0187,  0.1713, -0.2944]],\n",
      "\n",
      "        [[-0.3521,  0.1026, -0.2971]],\n",
      "\n",
      "        [[-0.3191,  0.0781, -0.1957]],\n",
      "\n",
      "        [[-0.1634,  0.0941, -0.1637]],\n",
      "\n",
      "        [[-0.3368,  0.0959, -0.0538]]])\n",
      "(tensor([[[-0.3368,  0.0959, -0.0538]]]), tensor([[[-0.9825,  0.4715, -0.0633]]]))\n"
     ]
    }
   ],
   "source": [
    "# Sequence 전체를 한 번에 진행해도 똑같은 결과를 얻을 수 있다.\n",
    "# LSTM이 주는 output 중 첫 번째 값인 \"out\"은 sequence 모두를 진행하면서 얻게 되는\n",
    "# 모든 hidden state를 담고 있다.\n",
    "# 두 번째 output인 \"hidden\"은 가장 최근의 hidden state만을 갖고 있다.\n",
    "# (\"out\"의 마지막과 \"hidden\"의 값이 같은 것을 확인해보자)\n",
    "# \"out\"을 통해 sequence 전체의 hidden state에 접근할 수 있는 것에 가치가 있고,\n",
    "# \"hidden\"은 LSTM의 argument로 쓰여서 sequence의 backpropagate를 계속 진행할 수 있게 해주는 것이다.\n",
    "\n",
    "# LSTM에 넣기 위해 두 번째 축을 추가해서 3D tensor로 만들자.\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "# Hidden state를 초기화한다.\n",
    "hidden = (torch.autograd.Variable(torch.randn(1, 1, 3)),\n",
    "          torch.autograd.Variable(torch.randn((1, 1, 3))))\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM for part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input 문장을 w1,⋯,wM 이라고 한다.\n",
    "1) wi∈V, 즉 각 단어 wi는 단어장 V 안에 속해있다.\n",
    "2) T를 품사 모음으로 표시한다. 그리고 yi를 단어 wi의 품사로 표시한다.\n",
    "3) 이제 단어 wi의 품사에 대한 우리의 예측을 y^i로 표시한다.\n",
    "\n",
    "- 이제 예측을 하기 위해서 LSTM 모델에 문장을 던져줘야 한다. \n",
    "- i 번째 순서의 hidden state를 hi라고 표시하겠다. \n",
    "- 그리고 모든 품사 태그에 고유한 숫자를 부여하겠다. \n",
    "- word embedding 예제에서 word_to_ix를 만들었던 것과 똑같은 이치이다. \n",
    "- y^i를 계산하기 위한 규칙은 다음과 같다.\n",
    "\n",
    "- y^i=argmaxj(logSoftmax(Ahi+b))j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "data = pd.read_csv('./nsmc/example.csv', encoding = 'cp949')\n",
    "ans, pos = data['answer'], data['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ans_text = ans[0].split('.')\n",
    "pos_text = pos[0].split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Noun, Verb, Adjective, Else\n",
    "korean = list()\n",
    "entity = list()\n",
    "for elem in pos_text:\n",
    "    elem_split = elem.split(' ')\n",
    "    sub_entity = list()\n",
    "    sub_korean = list()\n",
    "    for sub_elem in elem_split:\n",
    "        sub_elem_split = sub_elem.split('/')\n",
    "        if len(sub_elem_split) > 1:\n",
    "            sub_korean.append(sub_elem_split[0])\n",
    "            if sub_elem_split[1] in ['Noun','Verb','Adjective']:\n",
    "                sub_entity.append(sub_elem_split[1])\n",
    "            else:\n",
    "                sub_entity.append('Else')\n",
    "    korean.append(sub_korean)\n",
    "    entity.append(sub_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "korean_tag = list(Counter(sum(entity,[])).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return torch.autograd.Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'리더': 0, '의': 1, '책임감': 2, '과': 3, '학과': 4, '1': 5, '년': 6, '을': 7, '잘': 8, '경영': 9, '해야': 10, '겠': 11, '다는': 12, '열정': 13, '으로': 14, '문제점': 15, '파악하고': 16, '방안': 17, '마련한': 18, '경험': 19, '이': 20, '있': 21, '습니다': 22, '': 23, '때': 24, '대다수': 25, '구성원': 26, '들': 27, '설득': 28, '했': 29, '던': 30, '과정': 31, '기억': 32, '에': 33, '많이': 34, '남': 35}\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    (korean[0], entity[0]),\n",
    "    (korean[1], entity[1]),\n",
    "]\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"Else\": 0, \"Noun\": 1, \"Verb\": 2, \"Adjective\" : 3}\n",
    "\n",
    "# 아래 값들은 보통 32 또는 64 차원으로 사용하지만\n",
    "# 여기서는 우리가 train하는 weight들이 어떻게 변하는지 직접 볼 수 있도록\n",
    "# 작게 설정한다.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size,\n",
    "                                                  embedding_dim)\n",
    "        \n",
    "        # LSTM은 word embedding과 hidden 차원값을 input으로 받고,\n",
    "        # hidden state를 output으로 내보낸다.\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Hidden state space에서 tag space로 보내는 linear layer를 준비한다.\n",
    "        self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # Hidden state는 자동적으로 만들어지지 않으므로 직접 기능을 만들겠다.\n",
    "        # 3D tensor의 차원은 각각 (layer 개수, mini-batch 개수, hidden 차원)\n",
    "        # 을 의미한다. 왜 이렇게 해야만 하는지 궁금하다면 Pytorch 문서를 참고 바란다.\n",
    "        return (\n",
    "            torch.autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "            torch.autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "        )\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1),\n",
    "            self.hidden\n",
    "        )\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = torch.nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4375, -1.4504, -1.6374, -1.0980],\n",
      "        [-1.4177, -1.4590, -1.5461, -1.1641],\n",
      "        [-1.3452, -1.3871, -1.6143, -1.2356],\n",
      "        [-1.3194, -1.3969, -1.6079, -1.2551],\n",
      "        [-1.2656, -1.4696, -1.6918, -1.1917],\n",
      "        [-1.4048, -1.4731, -1.5217, -1.1808],\n",
      "        [-1.3910, -1.3842, -1.5444, -1.2476],\n",
      "        [-1.3512, -1.3893, -1.5865, -1.2476],\n",
      "        [-1.3708, -1.3502, -1.5670, -1.2793],\n",
      "        [-1.3624, -1.3672, -1.5820, -1.2602],\n",
      "        [-1.2968, -1.3721, -1.6458, -1.2724],\n",
      "        [-1.2338, -1.4749, -1.6644, -1.2353],\n",
      "        [-1.4441, -1.4147, -1.5423, -1.1804],\n",
      "        [-1.4133, -1.4408, -1.6053, -1.1422],\n",
      "        [-1.3752, -1.4578, -1.5350, -1.2073],\n",
      "        [-1.4161, -1.4195, -1.5363, -1.2029],\n",
      "        [-1.3583, -1.4408, -1.5634, -1.2149],\n",
      "        [-1.3898, -1.3574, -1.5578, -1.2626],\n",
      "        [-1.3647, -1.3308, -1.6248, -1.2612],\n",
      "        [-1.3178, -1.3701, -1.5977, -1.2880],\n",
      "        [-1.3256, -1.3452, -1.6077, -1.2964],\n",
      "        [-1.4257, -1.3959, -1.5892, -1.1778],\n",
      "        [-1.3543, -1.3646, -1.6281, -1.2376],\n",
      "        [-1.2998, -1.4413, -1.5408, -1.2852],\n",
      "        [-1.3119, -1.4063, -1.6257, -1.2418],\n",
      "        [-1.3724, -1.4354, -1.5981, -1.1834]])\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM,\n",
    "                   len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training하기 전의 품사 태깅 점수를 보겠다.\n",
    "# Output의 (i, j) 원소는 i번째 단어가 j번째 품사일 점수를 나타낸다.\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['리더', '의', '책임감', '과', '학과', '의', '1', '년', '을', '잘', '경영', '해야', '겠', '다는', '열정', '으로', '문제점', '을', '파악하고', '방안', '을', '마련한', '경험', '이', '있', '습니다']\n",
      "tensor([[-0.2837, -2.3610, -1.9290, -4.9097],\n",
      "        [-0.2307, -2.9165, -1.9193, -5.2582],\n",
      "        [-1.9291, -0.2844, -2.3952, -4.5035],\n",
      "        [-1.1238, -0.5878, -2.3109, -3.9005],\n",
      "        [-1.1196, -0.7001, -1.8537, -3.8936],\n",
      "        [-0.2659, -2.5938, -1.8952, -4.7776],\n",
      "        [-0.3417, -2.0744, -1.8651, -4.7228],\n",
      "        [-1.5112, -0.4672, -1.9949, -4.0996],\n",
      "        [-0.2597, -3.0601, -1.7608, -4.6140],\n",
      "        [-0.6587, -1.5621, -1.3568, -4.1788],\n",
      "        [-2.6984, -0.1783, -2.4711, -4.4636],\n",
      "        [-1.0979, -0.8381, -1.5211, -4.1702],\n",
      "        [-0.2656, -2.8979, -1.7653, -4.9607],\n",
      "        [-0.5384, -1.6394, -1.5536, -4.5322],\n",
      "        [-1.2816, -0.5753, -1.9461, -4.0717],\n",
      "        [-0.3253, -2.0990, -1.9053, -5.0678],\n",
      "        [-1.4343, -0.5244, -1.8670, -4.1836],\n",
      "        [-0.2649, -3.0814, -1.7315, -4.6226],\n",
      "        [-0.8185, -1.4481, -1.2080, -3.6853],\n",
      "        [-1.2380, -0.6161, -1.8754, -4.0929],\n",
      "        [-0.2561, -3.1423, -1.7505, -4.7001],\n",
      "        [-0.2944, -2.9085, -1.6370, -5.1324],\n",
      "        [-2.3357, -0.2422, -2.2489, -4.3554],\n",
      "        [-0.5413, -1.1670, -2.4875, -3.7480],\n",
      "        [-1.2938, -0.6736, -1.9080, -2.6953],\n",
      "        [-0.4243, -1.6783, -1.9643, -3.9717]])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(300):\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Torch에서 gradient는 축적된다는 기억하자.\n",
    "        # 새로운 데이터를 넣기 전에, 기존 gradient 정보를 날려줘야 한다.\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # 또한 LSTM의 이전 단계 hidden state와 분리시키면서\n",
    "        # hidden state를 초기화해줘야 한다.\n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        # Step 2. Network에 넣을 수 있도록 input 자료를 알맞게 변환해준다.\n",
    "        # 즉, 단어 인덱스에 맞게 Variable로 변환해준다.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        \n",
    "        # Step 3. Forward pass를 돌려라.\n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        # Step 4. Loss, gradient를 계산하고,\n",
    "        # optimizer.step()을 통해 parameter를 업데이트한다.\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Training이 끝난 후의 결과를 보겠다.\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "print(training_data[0][0])\n",
    "tag_scores = model(inputs)\n",
    "# 출력물의 (i,j)번째 원소는 i번째 단어가 j번째 품사일 점수이다.\n",
    "# 그 중 최고 점수를 기록한 품사를 선택하게 된다.\n",
    "print(tag_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
