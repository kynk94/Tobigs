{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSMC language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import torchtext\n",
    "from konlpy.tag import Mecab\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset, Dataset\n",
    "import os\n",
    "\n",
    "from cnn_model import CNNClassifier\n",
    "from rnn_model import RNN\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리를 위해 torchtext 라이브러리를 사용한다. 텍스트를 전처리해서 단어셋 구축과 숫자화(numericalize) 역할까지 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/Users/kwang/Project/ToBigs/2018-2/Lecture_NLP/complete' \n",
    "tagger = Mecab()\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = 0 if USE_CUDA else -1 #DEVICE = 'cuda' if USE_CUDA else 'cpu'\n",
    "\n",
    "def pad_under_five(toknized):\n",
    "    \"\"\"\n",
    "    모델에서 5-gram 단위 필터를 사용하기 때문에\n",
    "    5-gram이 안되는 문장에 <pad>로 채워준다\n",
    "    \"\"\"\n",
    "    if len(toknized) < 5:\n",
    "        toknized.extend([\"<pad>\"]*(5-len(toknized)))\n",
    "    return toknized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한글 형태소 분석기로 mecab(konlpy)를 이용한다. windows 는 kkma 등 사용하시면 됩니다.\n",
    "\n",
    "최소 문장 길이만큼 패딩해주는 전처리 함수를 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(tokenize=tagger.morphs, lower=True, init_token=\"<s>\",\n",
    "             eos_token=\"</s>\", include_lengths=True,\n",
    "             batch_first=True, preprocessing=pad_under_five) \n",
    "\n",
    "LABEL = Field(sequential=False,use_vocab=True,unk_token=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트와 정답을 Field로 정의한다. 텍스트는 mecab 형태소 단위로 분리되며 패딩 전처리 함수 또한 여기서 정의한다.  \n",
    "\n",
    "라벨은 use_vocab 파라미터를 사용해서 일반 텍스트도 정답으로 사용될 수 있다.  \n",
    "\n",
    "Field 내에는 vocab이라는 객체가 정의되어 있다. 다만, Field에 실제 데이터를 할당한 이후부터 사용 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 레벨 문장의 길이가 1 이상인 경우만 허용\n",
    "train_data, test_data = TabularDataset.splits(path=DATA_PATH+'/nsmc/',\n",
    "                                              train='ratings_train.txt',\n",
    "                                              test='ratings_test.txt',\n",
    "                                              format='tsv', \n",
    "                                              skip_header=True, \n",
    "                                              fields=[('id',None),('text',TEXT),('label',LABEL)], \n",
    "                                              filter_pred = lambda x: True if len(x.text) > 1 else False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TabularDataset은 실제 텍스트 파일(json, txt, csv 등)을 읽어 field에 데이터를 할당하는 역할을 한다.\n",
    "\n",
    "이제 TEXT와 LABEL에 실제 데이터가 할당 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000 50000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, min_freq=2)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build_vocab이라는 메소드를 이용해서 vocab을 구축할 수 있다. 최소 빈도를 지정하여 코퍼스 구축이 가능하다.  \n",
    "\n",
    "이제 단어셋을 아래처럼 찍어볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29976"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '<s>', '</s>', '.', '이', '는', '영화', '다', '고', '하', '도', '의', '가', '은', '에', '을', '보', '한', '..', '게', ',', '들', '!', '지', '를', '있', '없', '?', '좋', '나', '었', '만', '는데', '너무', '봤', '적', '안', '정말', '로', '음', '으로', '것', '아', '네요', '재밌', '점', '어', '같', '지만', '진짜', '했', '에서', '기', '네', '않', '거', '았', '수', '되', '면', '과', '말', '연기', '인', '주', '잘', '최고', '~', '내', '평점', '이런', '던', '어요', '와', '생각', 'ㅎ', '할', '왜', '1', '겠', '스토리', '습니다', '해', '...', '드라마', '아니', '싶', '그', '사람', '듯', '함', '더', '감동', '때', '배우', '본', '까지', '좀', '뭐'] 29976\n"
     ]
    }
   ],
   "source": [
    "print (TEXT.vocab.itos[:100], len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset 객체에서 example 멤버를 이용하면 전처리된 실제 데이터에 순차적으로 접근할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아', '더빙', '.', '.', '진짜', '짜증', '나', '네요', '목소리'] 0\n",
      "['걍인피니트가짱이다', '.', '진짜', '짱', '이', '다', '♥'] 1\n",
      "['신카이', '마코토', '의', '작화', '와', ',', '미유', '와', '하나', '카', '나', '가', '연기', '를', '잘', '해', '줘서', '더', '대박', '이', '였', '다', '.'] 1\n"
     ]
    }
   ],
   "source": [
    "print (train_data.examples[0].text, train_data.examples[0].label)\n",
    "print (train_data.examples[10].text, train_data.examples[10].label)\n",
    "print (train_data.examples[100].text, train_data.examples[100].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 160207\n",
      "0 4\n",
      "<unk> <s>\n",
      "54802 29976 29976\n"
     ]
    }
   ],
   "source": [
    "print (TEXT.vocab.freqs['<unk>'], TEXT.vocab.freqs['.'])\n",
    "print (TEXT.vocab.stoi['<unk>'], TEXT.vocab.stoi['.'])\n",
    "print (TEXT.vocab.itos[0], TEXT.vocab.itos[2])\n",
    "print (len(TEXT.vocab.freqs), len(TEXT.vocab.itos), len(TEXT.vocab.stoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab 멤버로는 freq, stoi, itos가 있으며 각각 빈도, string to int, int to string을 나타낸다.  \n",
    "\n",
    "build_vocab에서 freq 제한을 두었기 때문에 실제 구축된 vocab의 길이가 freq 리스트의 길이보다 짧은 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset으로 읽은 데이터를 이용해서 연산 가능한 형태로 만든다. iterator화 해서 학습에 유용한 loader로 변환한다.  \n",
    "\n",
    "loader는 batch 단위로 접근이 가능하다. 문장은 numericalized된 long type 데이터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = BucketIterator.splits((train_data, test_data),  \n",
    "                                                  sort_key=lambda x:len(x.text),\n",
    "                                                  sort_within_batch=True,\n",
    "                                                  repeat=False,shuffle=True,\n",
    "                                                  batch_size=32,device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[     2,     38,     93,    202,     57,     82,    998,     17,\n",
      "              9,    746,    595,     51,     82,     71,     29,     14,\n",
      "              7,    471,     41,     11,    338,   5080,    732,    212,\n",
      "           1217,     80,     82,    765,      3],\n",
      "        [     2,    356,     98,  11524,     13,     78,    170,     98,\n",
      "          11524,     39,   3570,    198,      4,      4,    232,      5,\n",
      "             98,   1980,     49,   3209,   3245,     16,     94,     13,\n",
      "             27,      8,      4,    230,      3],\n",
      "        [     2,     40,      4,     19,    502,   1151,     28,     28,\n",
      "              7,     25,    895,     39,    287,     14,    172,     28,\n",
      "          11741,   1769,     22,     14,   4793,    103,     32,     14,\n",
      "             77,     90,      4,      4,      3],\n",
      "        [     2,    375,     10,     20,    188,    236,     15,    327,\n",
      "          15816,     74,    959,     20,     35,     72,      7,    146,\n",
      "              4,   2298,     61,    257,     61,    391,     61,    188,\n",
      "             89,     22,      4,     84,      3],\n",
      "        [     2,    125,    165,     14,      4,      4,   1244,      7,\n",
      "           1887,    108,    649,    117,     72,     42,     48,      8,\n",
      "              4,      4,    330,      4,      4,     79,    165,    399,\n",
      "             14,     51,      8,      4,      3],\n",
      "        [     2,   7302,    150,     12,   1496,    368,     15,   6242,\n",
      "           1011,     12,   1173,     61,    644,    173,     16,   2573,\n",
      "             10,     20,    153,     66,    822,     57,     44,   1118,\n",
      "             25,   1790,    206,      4,      3],\n",
      "        [     2,   4001,   5155,      8,   5155,    923,   5155,     32,\n",
      "              5,     77,     58,     26,      6,  10675,   8123,     23,\n",
      "             23,   3802,     11,   1872,     39,   5591,     58,     26,\n",
      "              6,   2525,     68,   1809,      3],\n",
      "        [     2,   1586,     70,    125,   5105,     97,    103,   2222,\n",
      "            169,     99,    121,     50,      4,      4,     43,  24173,\n",
      "            175,     30,   3736,    432,     22,      5,    154,     30,\n",
      "             28,   1475,      4,      4,      3],\n",
      "        [     2,    169,     99,     13,    240,      7,     28,  15346,\n",
      "           1967,     14,   1119,    186,    132,   2396,     47,    203,\n",
      "              9,      4,    531,     14,   3586,    348,    135,    100,\n",
      "             57,      8,    227,      3,      1],\n",
      "        [     2,   1495,    178,     12,    764,     63,      6,  27182,\n",
      "              4,     19,    395,   1404,     16,    517,    125,     46,\n",
      "            606,     10,     20,    474,      4,    295,     68,    340,\n",
      "            138,     64,     90,      3,      1],\n",
      "        [     2,   2652,     30,    143,      5,     86,    107,   1047,\n",
      "             61,   3023,     15,    309,   1120,     36,     64,      7,\n",
      "            144,      4,  10670,     25,    147,     10,     60,     17,\n",
      "             24,    293,   1075,      3,      1],\n",
      "        [     2,    945,     97,    523,   1671,      9,    312,    104,\n",
      "            183,    585,   3747,     42,     48,     14,      7,    794,\n",
      "            577,     83,     24,      9,    945,    201,      0,    599,\n",
      "             39,     96,      7,      3,      1],\n",
      "        [     2,    125,    165,    492,     63,      4,      4,    215,\n",
      "           1711,   1905,    534,    287,    198,    162,      9,    362,\n",
      "             32,      4,     19,   3389,     79,     68,    145,    165,\n",
      "             14,    103,     32,      3,      1],\n",
      "        [     2,     29,     43,     10,      6,   1021,    290,    113,\n",
      "            132,     21,    529,     10,      9,    120,    528,   1272,\n",
      "             21,   5271,    526,     16,     58,     27,     16,     56,\n",
      "            127,      4,      4,      3,      1],\n",
      "        [     2,    175,     22,      5,     34,    779,    299,     10,\n",
      "              6,     42,     48,     43,  17364,  17364,     91,    209,\n",
      "          21631,    676,    382,    132,    287,      6,     42,     48,\n",
      "             14,   3898,   2702,      3,      1],\n",
      "        [     2,    831,     12,    636,     14,    352,     10,      8,\n",
      "            576,    123,     62,     10,      6,     42,    770,      8,\n",
      "           3361,      5,      8,   2510,      6,  12247,    214,   2789,\n",
      "           1055,    276,  16095,      3,      1],\n",
      "        [     2,    389,     15,     11,     71,      7,   3350,     20,\n",
      "           1076,     77,   2170,      4,    105,      5,     95,    225,\n",
      "           9373,   1123,    631,    555,     10,     53,    507,     90,\n",
      "             87,      8,      4,      3,      1],\n",
      "        [     2,    658,     36,     64,      7,      8,      4,      4,\n",
      "            379,   1142,     53,      6,    430,      7,     25,    960,\n",
      "              6,      5,     22,      5,    404,     18,    129,    721,\n",
      "             17,    171,    212,      3,      1],\n",
      "        [     2,    141,     15,    131,      6,    190,    150,     14,\n",
      "              4,     61,   3391,   1231,    287,     72,  15293,    146,\n",
      "            592,   3987,      5,  10498,   5460,      5,    746,   4912,\n",
      "            193,    146,      4,      3,      1],\n",
      "        [     2,     69,     13,   1090,     18,    149,   1675,      4,\n",
      "              4,    105,    512,    708,     13,    192,     18,    149,\n",
      "           1675,      4,     19,   1623,    105,    108,     92,   1142,\n",
      "             54,      4,     19,      3,      1],\n",
      "        [     2,   6917,     15,      0,   1937,     18,    149,     99,\n",
      "            124,     21,     21,   5379,   1400,      5,   1005,     39,\n",
      "           6345,    121,      0,    628,      9,     87,     47,     28,\n",
      "           9702,    629,     15,      3,      1],\n",
      "        [     2,    537,    114,   2983,    526,      9,    210,      8,\n",
      "           1764,    123,   3851,     33,    110,    351,      8,   1085,\n",
      "             15,    307,    114,    627,     15,    130,   7876,     87,\n",
      "              8,      4,     19,      3,      1],\n",
      "        [     2,  15680,    138,     39,   6141,      5,     27,      8,\n",
      "              4,    837,    104,     15,     79,     46,   1446,    903,\n",
      "             10,    226,    535,     21,     92,   7404,     25,   1096,\n",
      "             17,     57,   1773,      3,      1],\n",
      "        [     2,    365,     13,     96,     42,    113,     15,     67,\n",
      "            117,     73,     23,  25310,     12,    837,    224,      4,\n",
      "              4,     17,      6,    574,    466,    282,     32,    392,\n",
      "             82,      4,      4,      3,      1],\n",
      "        [     2,   6226,      5,    221,      4,      4,    191,   1852,\n",
      "              6,    403,     25,     66,    449,     59,    311,      4,\n",
      "              4,   1462,    108,    403,   1499,      5,     67,     64,\n",
      "              4,     19,   6226,      3,      1],\n",
      "        [     2,    169,     78,    340,     46,    157,      7,     64,\n",
      "            135,    162,    605,      8,   2206,   2444,    316,     26,\n",
      "             33,   9443,     25,    147,    136,    604,     16,   1990,\n",
      "             57,     30,     28,      3,      1],\n",
      "        [     2,   3372,      8,      4,      4,    465,    108,     11,\n",
      "           1735,     83,   3183,    341,   2306,     64,   1223,     13,\n",
      "            748,  10759,      5,      9,   2625,     14,   3681,   6090,\n",
      "            482,    571,      0,      3,      1],\n",
      "        [     2,   1120,     36,     64,   1313,     21,   6724,    101,\n",
      "             10,  15250,     11,    162,     80,     49,   1335,    336,\n",
      "             77,     58,     27,     16,    138,     12,    464,  21599,\n",
      "           3683,     82,      4,      3,      1],\n",
      "        [     2,    510,     64,    149,    268,     49,      4,     19,\n",
      "             79,    165,     61,    145,    165,     15,    309,   1438,\n",
      "           4204,    138,      4,      4,     39,     75,     83,    203,\n",
      "            186,   1420,    329,      3,      1],\n",
      "        [     2,   1106,      4,    647,    138,   6054,     10,      9,\n",
      "             17,     24,     55,   2242,      6,    112,      5,    583,\n",
      "           1862,     58,     26,     40,      4,   1186,      5,   2847,\n",
      "              6,    291,   1668,      3,      1],\n",
      "        [     2,    161,     45,      6,     85,     13,   6937,     52,\n",
      "           1893,     59,      9,    719,    123,    894,     53,     32,\n",
      "             10,      8,      4,   2649,     12,     63,     13,   1113,\n",
      "              5,      8,      4,      3,      1],\n",
      "        [     2,   5349,    114,     11,    340,   1658,   3400,    166,\n",
      "             15,    309,    349,     52,   1039,    178,     61,   4436,\n",
      "            301,   7163,     13,    145,    112,  21861,    123,    531,\n",
      "             35,     24,      4,      3,      1]]), tensor([ 29,  29,  29,  29,  29,  29,  29,  29,  28,  28,  28,  28,\n",
      "         28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
      "         28,  28,  28,  28,  28,  28,  28,  28]))\n"
     ]
    }
   ],
   "source": [
    "for bt in train_loader:\n",
    "    print(bt.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size,embed_size,hidden_size,output_size,\n",
    "                 num_layers=1,bidirec=False,vocab=None):\n",
    "        super(RNN,self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        if bidirec:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "            \n",
    "        self.embed = nn.Embedding(input_size,embed_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size,hidden_size,num_layers,batch_first=True,bidirectional=bidirec)\n",
    "        self.linear = nn.Linear(hidden_size*self.num_directions,output_size)\n",
    "        #self.attention = Attention(self.hidden_size*self.num_directions, method='general')\n",
    "        \n",
    "    def init_hidden(self,batch_size):\n",
    "        # (num_layers * num_directions, batch_size, hidden_size)\n",
    "        \n",
    "        hidden = Variable(torch.zeros(self.num_layers*self.num_directions,batch_size,self.hidden_size)).cuda()\n",
    "        cell = Variable(torch.zeros(self.num_layers*self.num_directions,batch_size,self.hidden_size)).cuda()\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self,inputs, encoder_length=None):\n",
    "        \"\"\"\n",
    "        inputs : B,T\n",
    "        \"\"\"\n",
    "        embed = self.embed(inputs) # word vector indexing\n",
    "        hidden, cell = self.init_hidden(inputs.size(0)) # initial hidden,cell\n",
    "        \n",
    "        output, h = self.lstm(embed,(hidden,cell)) # output : Batch,Time,Hidden (32, 9, 200)\n",
    "        hidden, cell = h # hidden : L, B, H\n",
    "        \n",
    "        hidden = hidden[-self.num_directions:] # (num_directions,B,H)\n",
    "        hidden = torch.cat([h for h in hidden],1) #.unsqueeze(0) # (1,B,2H) (1, 32, 200)\n",
    "        \n",
    "        #print (output.size(), hidden.size(), len(encoder_length))\n",
    "        #print (encoder_length, type(encoder_length[0]))\n",
    "        \n",
    "        # Many-to-One\n",
    "        output = self.linear(hidden) # last hidden\n",
    "        output = output.squeeze(1)\n",
    "        \n",
    "        return F.log_softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 10\n",
    "BATCH_SIZE = 32\n",
    "EMBED = 200\n",
    "KERNEL_DIM = 100\n",
    "LR = 0.001\n",
    "output_size = 2\n",
    "\n",
    "# model = CNNClassifier(len(TEXT.vocab), EMBED, 1, KERNEL_DIM, KERNEL_SIZES)\n",
    "model = RNN(len(TEXT.vocab), EMBED, KERNEL_DIM, output_size, num_layers=2,\n",
    "            bidirec=True, vocab=TEXT.vocab)\n",
    "\n",
    "loss_function = nn.NLLLoss() # nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5], gamma=0.1)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train\n",
    "model.train()\n",
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    scheduler.step()\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        inputs, lengths = batch.text\n",
    "        targets = batch.label\n",
    "        # print (inputs.size())\n",
    "\n",
    "        if USE_CUDA:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        preds = model(inputs, encoder_length=lengths.tolist())\n",
    "        # print (preds, targets)\n",
    "        # print (preds.squeeze(1).size(), targets.size())\n",
    "\n",
    "        loss = loss_function(preds,targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # exit()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"epoch : %d mean_loss : %.3f , lr : %.5f\" % (epoch,np.mean(losses), scheduler.get_lr()[0]))\n",
    "            losses=[]\n",
    "\n",
    "torch.save(model.state_dict(), './model/nsmc_lm.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluate\n",
    "model.eval()\n",
    "num_hit=0\n",
    "\n",
    "for i,batch in enumerate(test_loader):\n",
    "    inputs, lengths = batch.text\n",
    "    targets = batch.label\n",
    "    \n",
    "#    print ('\\n',inputs.size())\n",
    "#    print (inputs)\n",
    "#    break\n",
    "\n",
    "    if USE_CUDA:\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "    output = model(inputs, lengths)\n",
    "\n",
    "    preds = output.max(1, keepdim=True)[1]\n",
    "#     print (preds.size())\n",
    "#     print (preds)\n",
    "#     print (targets)\n",
    "#     preds = preds*5\n",
    "#     preds = preds.round()\n",
    "    num_hit+=torch.eq(preds.squeeze(),targets.squeeze()).sum().item() \n",
    "\n",
    "print('test accuracy: %.4f%%'%(num_hit/len(test_data)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test\n",
    "test_inputs = [\"개별적인 장면이 좋았다.\", \"존맛탱!\", \"헐 진짜 개별로다..\", \n",
    "               \"진짜 너무 재밌는 영화다 오랜만에\",\"오..이건 진짜 봐야함\", \n",
    "               \"진짜 쓰레기 같은 영화\",\"노잼\",\"존잼\",\"꾸울잼\",\"핵노잼\",'또 보고싶다', \n",
    "               '꼬옥 봐야한다.. 진짜..', '나만 보기 아깝다', '돈이 아깝다', '나만 보기 억울하다', \n",
    "               '나만 당할 수 없다', '너도 봐야한다', '혼자 본게 정말 후회된다. 이건 꼭 같이 봐야한다.', \n",
    "               '재미없어요...', '꾸르르르르르르잼', '꾸르르르잼', '꾸르잼', '이 영화를 보고 암이 나았습니다.']  \n",
    "\n",
    "for test_input in test_inputs:\n",
    "    tokenized = tagger.morphs(test_input)\n",
    "    length = len(tokenized)\n",
    "    tokenized = pad_under_five(tokenized)\n",
    "    input_, lengths = TEXT.numericalize(([tokenized], length), device=DEVICE)\n",
    "    if USE_CUDA: input_ = input_.cuda()\n",
    "    \n",
    "#     output, attn_weights = model(input_, [lengths.tolist()])\n",
    "    output = model(input_, [lengths.tolist()])\n",
    "#     print (output)\n",
    "    prediction = output.max(1, keepdim=True)[1]\n",
    "#     print (prediction)\n",
    "\n",
    "    if prediction[0][0] == 1:\n",
    "        print(test_input,\"\\033[1;01;36m\" + '긍정' + \"\\033[0m\")\n",
    "    else:\n",
    "        print(test_input,\"\\033[1;01;31m\" + '부정' + \"\\033[0m\")\n",
    "    print ()\n",
    "    \n",
    "print (LABEL.vocab.itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(TEXT.vocab, './model/news_field.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (LABEL.numericalize(['스포츠']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocaburay = torch.load('./model/vocab.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (vocaburay.itos[:10])\n",
    "TEST = Field(tokenize=tagger.morphs,lower=True,include_lengths=False,batch_first=True,preprocessing=pad_under_five)\n",
    "TEST.build_vocab()\n",
    "TEST.vocab = vocaburay\n",
    "TEST.numericalize(test_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
